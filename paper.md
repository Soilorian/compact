---
title: 'COMPACT: Comprehensive Prefetching Algorithm Framework'
authors:
  - name: Mahdi Alinejad[^1]
    affiliation: '1'
  - name: Sahand Zoufan[^1]
    affiliation: '1'
  - name: Atiyeh Gheibi-Fetrat
    affiliation: '1'
  - name: Shaahin Hessabi
    affiliation: '1'
  - name: Hamid Sarbazi-Azad
    affiliation: '1,2'
affiliations:
  - index: 1
    name: Sharif University of Technology, Iran
  - index: 2
    name: IPM Institute For Research In Fundamental Sciences, Iran
date: 3 November 2025
bibliography: paper.bib
---

# Summary

COMPACT (Comprehensive Prefetching Algorithm Framework) is a Python-based simulation framework designed to support the rapid development and evaluation of hardware prefetching algorithms. It offers a modular and extensible environment that allows researchers and engineers to explore and test new prefetching techniques without requiring complex hardware changes. The framework includes several common prefetching algorithms, a flexible cache simulator, and built-in tools for analyzing the prefetcher performance. COMPACT is designed for ease of use and customization, making it a practical and valuable resource for the computer architecture research community.

# Statement of Need

As the performance gap between processors and main memory continues to grow, hardware prefetching has become a key approach to improving overall system performance. However, developing and evaluating new prefetching algorithms often requires significant time and resources. COMPACT helps address this issue by providing a lightweight and adaptable simulation framework that allows researchers to efficiently test and compare prefetching strategies. By simplifying the evaluation process, COMPACT supports faster experimentation and innovation in the study of hardware prefetching.


# Implemented Algorithms

The following prefetching algorithms are implemented in COMPACT. Brief descriptions are provided for each algorithm based on their original publications.

- **Markov Predictor** [@markovpredictor]: The Markov Predictor models memory access behavior by representing addresses as states in a Markov chain. Transition probabilities between states are learned from historical access patterns. During execution, the prefetcher predicts the most likely successor addresses based on the current state (accessed address), issuing prefetches for these predicted next addresses.

- **Correlation Prefetching** [@correlation]: The Correlation Prefetcher uses a helper memory thread that observes sequences of cache misses and learns correlations between them. It records recurring temporal miss patterns and uses these correlations to predict future memory accesses. When the thread detects a similar pattern, it replays the correlated sequence of misses to prefetch data ahead of demand accesses.

- **Dynamic Hot Data Stream Prefetching** [@hds]: The HDS prefetcher identifies frequently accessed memory streams by profiling bursty access patterns. These "hot" streams are modeled using deterministic finite state machines (DFSMs). During profiling, the most frequent sequences of accesses are encoded in DFSMs. At runtime, the prefetcher predicts future accesses by following the DFSM states, prefetching data ahead of demand accesses.

- **TCP** [@tcp]: The TCP Prefetcher operates by correlating cache line tags rather than full memory addresses. It maintains a history of recently accessed cache line tags and predicts the next tag based on these correlations. By chaining the predicted tags, TCP can prefetch the next set of cache lines ahead of the demand access. This technique focuses on the correlation of cache line tags, improving prefetch efficiency by predicting multiple cache lines at once.


- **Global History Buffer** [@ghb]: The GHB Prefetcher maintains a global history buffer of recent memory accesses and correlates the address deltas (the differences between consecutive memory addresses) across all program counters. It tracks the history of deltas and uses this information to predict future memory addresses based on recurring delta patterns. These predictions help issue prefetches for future memory accesses before they occur.

- **Store-Ordered Streaming** [@storeorderedstreamer]: The Store-Ordered Streamer Prefetcher records store streams generated by producers (e.g., stores to memory). When a consumer (e.g., a load) accesses a line that is part of a producer's store sequence, the prefetcher triggers a series of prefetches for subsequent cache lines in the producer’s sequence. This effectively streams data from a producer to a consumer even when their access orders differ.

- **Spatial Memory Streaming** [@sms]: The SMS Prefetcher tracks access patterns in memory regions using two main components: an accumulation table that records access history for each region, and a pattern history table that stores spatial signatures. When a memory region is accessed, its pattern signature is matched against stored patterns to prefetch future accesses within that region, improving spatial locality.

- **Epoch-Based Correlation Prefetching** [@ebcp]: The EBCP Prefetcher partitions the memory access stream into epochs, with boundaries defined by quiescence periods (idle times). It learns correlations between the first miss of an epoch and subsequent misses within that same epoch. Upon detection of a similar epoch trigger, the prefetcher replays the learned miss sequence to prefetch data ahead of demand.

- **Feedback-Directed Prefetching** [@feedbackdirected]: The Feedback-Directed Prefetcher monitors feedback metrics such as prefetch accuracy, lateness, and cache pollution. It dynamically adjusts prefetch parameters like prefetch degree based on these metrics over sampling intervals. This tuning process helps balance the trade-off between cache performance improvement and bandwidth usage.

- **Temporal Instruction Fetch Streaming** [@temporalmemorystreaming]: The Temporal Memory Streaming Prefetcher records instruction access sequences in a circular miss-order buffer. When an instruction is fetched, it checks the circular buffer for any matching address, and if a match is found, it prefetches subsequent instructions from the recorded sequence. A directory maps instruction addresses to sequence positions, allowing the prefetcher to maintain and replay instruction fetch sequences efficiently.

- **Linearizing Irregular Memory Accesses** [@linear]: The Linear Prefetcher transforms irregular memory access patterns into stream-like sequences by logically linearizing the access patterns. Non-sequential accesses are mapped into a format that makes them appear sequential, allowing conventional prefetching mechanisms to operate effectively. This approach enables the application of standard prefetching techniques to irregular patterns.

- **B-Fetch** [@bfetch]: The B-fetch Prefetcher uses branch prediction information to guide instruction prefetching in chip multiprocessors. By following predicted branch outcomes and targets, it prefetches instructions along likely future control-flow paths rather than relying only on past fetch history. This approach improves prefetch accuracy and timeliness for workloads with irregular control flow.

- **IMP** [@indirectmemory]: The Indirect Memory Prefetcher detects indirect memory access patterns where future addresses depend on previous load values (e.g., pointer-based or indexed accesses). By learning the correlation between index values and their corresponding target addresses, the prefetcher predicts future accesses based on the learned index-to-target relationship.

- **Best-Offset Hardware Prefetching** [@bestoffset]: The Best-Offset Prefetcher evaluates a predefined set of candidate address offsets and tracks the usefulness of each in generating prefetches. The prefetcher selects the highest-scoring offset based on periodic evaluation and continuously adapts to changes in program access behavior by re-evaluating the offsets.

- **Efficient Footprint Caching** [@f_tdc_prefetcher]: The F-TDC Prefetcher learns spatial access footprints at the page level using compact footprint vectors. It predicts future memory accesses within a page by proactively fetching cache blocks associated with a previously accessed page, which is particularly useful in tagless DRAM cache designs.

- **Graph Prefetching** [@graph]: The Graph Prefetcher utilizes programmer- or compiler-provided graph traversal semantics to guide prefetching. It identifies memory access patterns corresponding to graph traversal operations and prefetches connected nodes ahead of traversal, optimizing access to irregular graph data structures.

- **Translation-Triggered Prefetching** [@tempo]: The TEMPO Prefetcher leverages address translation activity as a trigger for prefetching. It combines physical page mappings with cache-line offsets to predict future memory accesses. Non-speculative prefetches are issued as part of the address translation process, allowing better overlap between memory latency and translation.

- **Domino Temporal Data Prefetcher** [@domino]:The Domino Prefetcher uses two history tables to capture first-order and second-order correlations between cache misses. Predictions from these tables are chained together, providing more accurate prefetching by capturing deeper temporal relationships. This technique builds on the idea of "domino" effects, where early misses influence later prefetches.

- **Event-Triggered Programmable Prefetcher** [@eventtriggered]: The Event-Triggered Prefetcher is programmable, where prefetching occurs only when specific events—defined by the programmer or system—are triggered based on runtime memory access characteristics (e.g., stride, latency, timing). This allows the prefetching strategy to be adapted dynamically to the workload characteristics.

- **LearnPrefetch Prefetcher** [@hashemi2018]: The LearnPrefetch Prefetcher uses an LSTM model to learn long-range temporal dependencies from address delta sequences derived from cache misses. By training on historical memory miss sequences, the LSTM predicts future address deltas and generates prefetch candidates, capturing complex temporal access patterns.

- **LearnCluster Prefetcher** [@hashemi2018]: An extension of the LearnPrefetch, the LearnCluster Prefetcher clusters memory accesses using k-means clustering. Each cluster learns its own set of address deltas using the shared LSTM encoder. This improves prediction accuracy by specializing in different access patterns across clusters.

- **Bingo Spatial Data Prefetcher** [@bingo]: The Bingo Prefetcher tracks memory access patterns within regions using bit-vector footprints. It associates these footprints with specific trigger events. When a trigger matches a stored footprint, prefetches are issued for all cache blocks within the region indicated by the matched footprint.

- **DSPatch** [@dspatch]: The DSPatch Prefetcher maintains two types of footprints: accuracy-biased and coverage-biased patterns for each spatial signature. Depending on the access behavior, it selects the appropriate pattern to balance accuracy and coverage. Prefetches are issued by aligning the selected footprint with the region’s trigger offset.


- **Efficient Metadata Management** [@metadata]: The Metadata Prefetcher maintains per-PC metadata to track correlations between trigger addresses and their predicted target addresses. Confidence counters are associated with each correlation, and prefetches are only issued when these counters exceed a defined threshold, reducing the overhead of ineffective prefetches.

- **Perceptron-Based Prefetch Filtering** [@perceptron]: The Perceptron Prefetcher uses a perceptron classifier to filter prefetch candidates by evaluating features extracted from the memory access stream. The perceptron is trained online using feedback from previous prefetch decisions, reinforcing high-confidence prefetches and suppressing low-confidence ones to reduce ineffective prefetches and cache pollution.

- **Temporal Prefetching Without the Off-Chip Metadata** [@triage]: The Triage Prefetcher operates with on-chip metadata that tracks temporal correlations between consecutive memory addresses. The metadata cache is adaptively sized based on observed prefetch effectiveness, and prefetches are issued when high-confidence temporal correlations are identified.

- **IPCP Prefetcher** [@ipcp]: The IPCP Prefetcher classifies memory access patterns based on program counters (PCs) into constant stride, spatial signatures, or complex patterns. It uses specialized sub-prefetchers for each classification, allowing the prefetching strategy to adapt to different types of memory access patterns.

- **Hierarchical Neural Model of Data Prefetching** [@neural]: The Neural Prefetcher combines a global LSTM model for capturing long-range memory access patterns with per-PC linear models to learn localized instruction-specific behaviors. This hierarchical structure enables the prefetcher to adapt to both global trends and local, per-instruction access patterns.

**Triangel Prefetcher** [@triangel]: The Triangel Prefetcher uses both pattern-based prediction and neighbor-based prediction. It identifies recurring access patterns through training and tracks Markov-style temporal metadata to predict neighboring address relationships. This hybrid approach allows the prefetcher to optimize both accuracy and timeliness in temporal prefetching.

# References

- [@markovpredictor]
- [@correlation]
- [@hds]
- [@tcp]
- [@ghb]
- [@storeorderedstreamer]
- [@sms]
- [@ebcp]
- [@feedbackdirected]
- [@temporalmemorystreaming]
- [@linear]
- [@bfetch]
- [@indirectmemory]
- [@bestoffset]
- [@f_tdc_prefetcher]
- [@graph]
- [@tempo]
- [@domino]
- [@eventtriggered]
- [@hashemi2018]
- [@bingo]
- [@dspatch]
- [@metadata]
- [@perceptron]
- [@triage]
- [@ipcp]
- [@neural]
- [@triangel]

[^1]: These authors contributed equally to this work.